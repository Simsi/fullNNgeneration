{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from torch import nn\n",
        "from tqdm import tqdm\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import Dataset, random_split\n"
      ],
      "metadata": {
        "id": "eF3A9-74IS9o"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cpu'\n",
        "epochs = 10\n",
        "lr = 0.00001\n",
        "bs = 32"
      ],
      "metadata": {
        "id": "X01wJHnzRyWH"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "zTPbirGs0vzM"
      },
      "outputs": [],
      "source": [
        "layers_set = {\n",
        "                # increase channels with kernel 3\n",
        "                'conv=channel_factor:2,kernel_size:3,stride:1,padding:0-',\n",
        "                'conv=channel_factor:3,kernel_size:3,stride:1,padding:0-',\n",
        "                'conv=channel_factor:4,kernel_size:3,stride:1,padding:0-',\n",
        "\n",
        "                # decrease channels with kernel 3\n",
        "                'conv=channel_factor:0.4,kernel_size:3,stride:1,padding:0-',\n",
        "                'conv=channel_factor:0.6,kernel_size:3,stride:1,padding:0-',\n",
        "                'conv=channel_factor:0.8,kernel_size:3,stride:1,padding:0-',\n",
        "\n",
        "\n",
        "                 'batchnorm=eps:0.00001-',\n",
        "                 'batchnorm=eps:0.0001-',\n",
        "\n",
        "                 'avgpool=kernel_size:2,stride:2,padding:0-',\n",
        "                 'avgpool=kernel_size:4,stride:4,padding:0-',\n",
        "\n",
        "                 'maxpool=kernel_size:2,stride:2,padding:0-',\n",
        "                 'maxpool=kernel_size:4,stride:4,padding:0-',\n",
        "\n",
        "                 'dropout=p:0.2-',\n",
        "                 'dropout=p:0.4-',\n",
        "  }"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_layers_string(min_len=2, max_len=10):\n",
        "    if min_len < 1:\n",
        "      print('min_len < 0,\\n Please, choose min_len >= 1')\n",
        "      return None\n",
        "    length = random.randint(min_len, max_len)\n",
        "    text = random.sample(layers_set, length)\n",
        "    text = ''.join((layer for layer in text))\n",
        "    return text"
      ],
      "metadata": {
        "id": "7Zoc5d48M1Uf"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NN(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels):\n",
        "      super().__init__()\n",
        "\n",
        "      self.layers = nn.Sequential()\n",
        "\n",
        "    def forward(self, x):\n",
        "      x = self.layers(x)\n",
        "      return x\n",
        "\n",
        "    def __call__(self, x):\n",
        "      return self.forward(x)"
      ],
      "metadata": {
        "id": "tir6hcEbITB5"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class nnGenerator():\n",
        "    def __init__(self):\n",
        "      self.text_layers_dict = dict({})\n",
        "      self.nn_len = -1\n",
        "\n",
        "    def parseTextNet(self, text_net):\n",
        "      self.text_layers_dict = dict({})\n",
        "      self.nn_len = -1\n",
        "      print(text_net)\n",
        "      if text_net[-1] == '-':\n",
        "        text_net = text_net[:-1]\n",
        "      text_layers = text_net.split('-')\n",
        "      id = 0\n",
        "      for text_layer in text_layers:\n",
        "        tmp = text_layer.split('=')\n",
        "        layer_name, layer_params = tmp[0], tmp[1].split(',')\n",
        "        layer_params_dict = dict({})\n",
        "        for param in layer_params:\n",
        "          param = param.split(':')\n",
        "          param_name, param_value = param[0], param[1]\n",
        "          layer_params_dict[param_name] = param_value\n",
        "        self.text_layers_dict[layer_name + str(id)] = (id, layer_params_dict)\n",
        "        id += 1\n",
        "      print(self.text_layers_dict)\n",
        "\n",
        "    def get_text_layers_dict(self):\n",
        "      return self.text_layers_dict\n",
        "\n",
        "    def get_nn_len(self):\n",
        "      return self.nn_len\n",
        "\n",
        "    def conv_output_shape(self, h, w, kernel_size=1, stride=1, pad=0, dilation=1):\n",
        "      h = math.floor( ((h + (2 * pad) - ( dilation * (kernel_size - 1) ) - 1 )/ stride) + 1)\n",
        "      w = math.floor( ((w + (2 * pad) - ( dilation * (kernel_size - 1) ) - 1 )/ stride) + 1)\n",
        "      return h, w\n",
        "\n",
        "    def generateNN(self, n_classes, test_batch):\n",
        "      success_state = False\n",
        "\n",
        "      backbone = nn.Sequential()\n",
        "      classifier = nn.Sequential()\n",
        "      optimizer = None\n",
        "      try:\n",
        "        data_shape = np.array(test_batch).shape # [B, C, H, W]\n",
        "        last_shape = data_shape\n",
        "        for layer_name in self.text_layers_dict.keys():\n",
        "            layer = None\n",
        "            layer_params= self.text_layers_dict[layer_name][1]\n",
        "            if layer_name.find('conv') >= 0:\n",
        "              kernel_size = int(layer_params['kernel_size'])\n",
        "              channel_factor = float(layer_params['channel_factor'])\n",
        "              stride = int(layer_params['stride'])\n",
        "              padding = int(layer_params['padding'])\n",
        "              dilation = 1\n",
        "\n",
        "              activation = nn.ReLU(inplace=True)\n",
        "\n",
        "              in_chan = last_shape[1]\n",
        "              assert(in_chan <= last_shape[2] and in_chan <= last_shape[3])\n",
        "              out_chan = math.floor(in_chan * channel_factor)\n",
        "              assert(out_chan > 0)\n",
        "\n",
        "              backbone.append(nn.Conv2d(in_chan, out_chan, kernel_size, stride, padding, dilation))\n",
        "              backbone.append(activation)\n",
        "\n",
        "              h, w = self.conv_output_shape(last_shape[2], last_shape[3], kernel_size, stride, padding, dilation)\n",
        "              last_shape = (last_shape[0], out_chan, h, w)\n",
        "\n",
        "            elif layer_name.find('batchnorm') >= 0:\n",
        "              eps = float(layer_params['eps'])\n",
        "              in_chan = last_shape[1]\n",
        "              backbone.append(nn.BatchNorm2d(in_chan, eps))\n",
        "\n",
        "            elif layer_name.find('avgpool') >= 0:\n",
        "              kernel_size = int(layer_params['kernel_size'])\n",
        "              stride = int(layer_params['stride'])\n",
        "              padding = int(layer_params['padding'])\n",
        "              dilation = 1\n",
        "              out_chan = last_shape[1]\n",
        "              backbone.append(nn.AvgPool2d(kernel_size, stride, padding))\n",
        "\n",
        "              h, w = self.conv_output_shape(last_shape[2], last_shape[3], kernel_size, stride, padding, dilation)\n",
        "              last_shape = (last_shape[0], out_chan, h, w)\n",
        "\n",
        "\n",
        "            elif layer_name.find('maxpool') >= 0:\n",
        "              kernel_size = int(layer_params['kernel_size'])\n",
        "              stride = int(layer_params['stride'])\n",
        "              padding = int(layer_params['padding'])\n",
        "              dilation = 1\n",
        "\n",
        "              backbone.append(nn.MaxPool2d(kernel_size, stride, padding))\n",
        "\n",
        "              h, w = self.conv_output_shape(last_shape[2], last_shape[3], kernel_size, stride, padding, dilation)\n",
        "              last_shape = (last_shape[0], last_shape[1], h, w)\n",
        "\n",
        "            elif layer_name.find('dropout') >= 0:\n",
        "              p = float(layer_params['p'])\n",
        "              backbone.append(nn.Dropout2d(p))\n",
        "\n",
        "        linear_in_shape = last_shape[1] * last_shape[2] * last_shape[3]\n",
        "        classifier = nn.Linear(linear_in_shape, n_classes)\n",
        "        success_state = True\n",
        "        print('NN build successfull!')\n",
        "\n",
        "      except Exception as e:\n",
        "        print('NN build failed!')\n",
        "        print(str(e))\n",
        "\n",
        "      net = nn.Sequential()\n",
        "      net.append(backbone)\n",
        "      net.append(nn.Flatten(start_dim=1))\n",
        "      net.append(classifier)\n",
        "      self.text_layers_dict = dict({})\n",
        "      return success_state, net"
      ],
      "metadata": {
        "id": "rxbbC5VzHv7t"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class ds(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "\n",
        "      self.X = X\n",
        "      self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x_ = self.X[idx]\n",
        "        y_ = self.y[idx]\n",
        "        return x_, y_\n",
        ""
      ],
      "metadata": {
        "id": "b_bggXBXLeee"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = dsets.MNIST(root = './data', train = True,\n",
        "                        transform = transforms.ToTensor(), download = True)\n",
        "\n",
        "test_data = dsets.MNIST(root = './data', train = False,\n",
        "                       transform = transforms.ToTensor())\n",
        "\n",
        "\n",
        "train_samples = np.expand_dims(np.array(train_data.data), axis=1)[:5000]\n",
        "train_labels = np.array(train_data.targets)[:5000]\n",
        "\n",
        "\n",
        "dataset = ds(X=train_samples, y=train_labels)\n",
        "\n",
        "train_set, valid_set = random_split(dataset, [0.8, 0.2], generator=torch.Generator().manual_seed(42))\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(\n",
        "  train_set,\n",
        "  batch_size=bs,\n",
        "  shuffle=True,\n",
        "  drop_last=True)\n",
        "\n",
        "valid_dataloader = torch.utils.data.DataLoader(\n",
        "  valid_set,\n",
        "  batch_size=bs,\n",
        "  drop_last=True,\n",
        "  shuffle=True)"
      ],
      "metadata": {
        "id": "s2tqJJTrLjR5"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_batch = None\n",
        "for b, _ in train_dataloader:\n",
        "  test_batch = b\n",
        "  break"
      ],
      "metadata": {
        "id": "5se08dBBMISM"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator = nnGenerator()"
      ],
      "metadata": {
        "id": "G6j7cO3UL3wQ"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "success_state = False\n",
        "while not success_state:\n",
        "  text_layers = create_layers_string()\n",
        "  generator.parseTextNet(text_layers)\n",
        "  success_state, seq = generator.generateNN(n_classes=10, test_batch=test_batch)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J364qjskTTBo",
        "outputId": "a977181d-4517-47aa-a265-eb3211373a31"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "conv=channel_factor:0.4,kernel_size:3,stride:1,padding:0-batchnorm=eps:0.00001-conv=channel_factor:4,kernel_size:3,stride:1,padding:0-conv=channel_factor:0.8,kernel_size:3,stride:1,padding:0-maxpool=kernel_size:4,stride:4,padding:0-\n",
            "{'conv0': (0, {'channel_factor': '0.4', 'kernel_size': '3', 'stride': '1', 'padding': '0'}), 'batchnorm1': (1, {'eps': '0.00001'}), 'conv2': (2, {'channel_factor': '4', 'kernel_size': '3', 'stride': '1', 'padding': '0'}), 'conv3': (3, {'channel_factor': '0.8', 'kernel_size': '3', 'stride': '1', 'padding': '0'}), 'maxpool4': (4, {'kernel_size': '4', 'stride': '4', 'padding': '0'})}\n",
            "NN build failed!\n",
            "\n",
            "batchnorm=eps:0.00001-conv=channel_factor:3,kernel_size:3,stride:1,padding:0-avgpool=kernel_size:4,stride:4,padding:0-conv=channel_factor:0.6,kernel_size:3,stride:1,padding:0-maxpool=kernel_size:4,stride:4,padding:0-dropout=p:0.2-conv=channel_factor:0.8,kernel_size:3,stride:1,padding:0-dropout=p:0.4-\n",
            "{'batchnorm0': (0, {'eps': '0.00001'}), 'conv1': (1, {'channel_factor': '3', 'kernel_size': '3', 'stride': '1', 'padding': '0'}), 'avgpool2': (2, {'kernel_size': '4', 'stride': '4', 'padding': '0'}), 'conv3': (3, {'channel_factor': '0.6', 'kernel_size': '3', 'stride': '1', 'padding': '0'}), 'maxpool4': (4, {'kernel_size': '4', 'stride': '4', 'padding': '0'}), 'dropout5': (5, {'p': '0.2'}), 'conv6': (6, {'channel_factor': '0.8', 'kernel_size': '3', 'stride': '1', 'padding': '0'}), 'dropout7': (7, {'p': '0.4'})}\n",
            "NN build failed!\n",
            "\n",
            "avgpool=kernel_size:4,stride:4,padding:0-conv=channel_factor:0.6,kernel_size:3,stride:1,padding:0-dropout=p:0.4-conv=channel_factor:0.8,kernel_size:3,stride:1,padding:0-\n",
            "{'avgpool0': (0, {'kernel_size': '4', 'stride': '4', 'padding': '0'}), 'conv1': (1, {'channel_factor': '0.6', 'kernel_size': '3', 'stride': '1', 'padding': '0'}), 'dropout2': (2, {'p': '0.4'}), 'conv3': (3, {'channel_factor': '0.8', 'kernel_size': '3', 'stride': '1', 'padding': '0'})}\n",
            "NN build failed!\n",
            "\n",
            "conv=channel_factor:0.6,kernel_size:3,stride:1,padding:0-avgpool=kernel_size:2,stride:2,padding:0-conv=channel_factor:0.8,kernel_size:3,stride:1,padding:0-conv=channel_factor:0.4,kernel_size:3,stride:1,padding:0-conv=channel_factor:4,kernel_size:3,stride:1,padding:0-conv=channel_factor:3,kernel_size:3,stride:1,padding:0-maxpool=kernel_size:4,stride:4,padding:0-batchnorm=eps:0.00001-maxpool=kernel_size:2,stride:2,padding:0-batchnorm=eps:0.0001-\n",
            "{'conv0': (0, {'channel_factor': '0.6', 'kernel_size': '3', 'stride': '1', 'padding': '0'}), 'avgpool1': (1, {'kernel_size': '2', 'stride': '2', 'padding': '0'}), 'conv2': (2, {'channel_factor': '0.8', 'kernel_size': '3', 'stride': '1', 'padding': '0'}), 'conv3': (3, {'channel_factor': '0.4', 'kernel_size': '3', 'stride': '1', 'padding': '0'}), 'conv4': (4, {'channel_factor': '4', 'kernel_size': '3', 'stride': '1', 'padding': '0'}), 'conv5': (5, {'channel_factor': '3', 'kernel_size': '3', 'stride': '1', 'padding': '0'}), 'maxpool6': (6, {'kernel_size': '4', 'stride': '4', 'padding': '0'}), 'batchnorm7': (7, {'eps': '0.00001'}), 'maxpool8': (8, {'kernel_size': '2', 'stride': '2', 'padding': '0'}), 'batchnorm9': (9, {'eps': '0.0001'})}\n",
            "NN build failed!\n",
            "\n",
            "conv=channel_factor:0.6,kernel_size:3,stride:1,padding:0-dropout=p:0.2-batchnorm=eps:0.00001-maxpool=kernel_size:4,stride:4,padding:0-avgpool=kernel_size:2,stride:2,padding:0-conv=channel_factor:0.4,kernel_size:3,stride:1,padding:0-conv=channel_factor:4,kernel_size:3,stride:1,padding:0-avgpool=kernel_size:4,stride:4,padding:0-conv=channel_factor:0.8,kernel_size:3,stride:1,padding:0-batchnorm=eps:0.0001-\n",
            "{'conv0': (0, {'channel_factor': '0.6', 'kernel_size': '3', 'stride': '1', 'padding': '0'}), 'dropout1': (1, {'p': '0.2'}), 'batchnorm2': (2, {'eps': '0.00001'}), 'maxpool3': (3, {'kernel_size': '4', 'stride': '4', 'padding': '0'}), 'avgpool4': (4, {'kernel_size': '2', 'stride': '2', 'padding': '0'}), 'conv5': (5, {'channel_factor': '0.4', 'kernel_size': '3', 'stride': '1', 'padding': '0'}), 'conv6': (6, {'channel_factor': '4', 'kernel_size': '3', 'stride': '1', 'padding': '0'}), 'avgpool7': (7, {'kernel_size': '4', 'stride': '4', 'padding': '0'}), 'conv8': (8, {'channel_factor': '0.8', 'kernel_size': '3', 'stride': '1', 'padding': '0'}), 'batchnorm9': (9, {'eps': '0.0001'})}\n",
            "NN build failed!\n",
            "\n",
            "batchnorm=eps:0.00001-dropout=p:0.4-batchnorm=eps:0.0001-conv=channel_factor:0.8,kernel_size:3,stride:1,padding:0-maxpool=kernel_size:4,stride:4,padding:0-conv=channel_factor:3,kernel_size:3,stride:1,padding:0-\n",
            "{'batchnorm0': (0, {'eps': '0.00001'}), 'dropout1': (1, {'p': '0.4'}), 'batchnorm2': (2, {'eps': '0.0001'}), 'conv3': (3, {'channel_factor': '0.8', 'kernel_size': '3', 'stride': '1', 'padding': '0'}), 'maxpool4': (4, {'kernel_size': '4', 'stride': '4', 'padding': '0'}), 'conv5': (5, {'channel_factor': '3', 'kernel_size': '3', 'stride': '1', 'padding': '0'})}\n",
            "NN build failed!\n",
            "\n",
            "dropout=p:0.2-conv=channel_factor:3,kernel_size:3,stride:1,padding:0-dropout=p:0.4-\n",
            "{'dropout0': (0, {'p': '0.2'}), 'conv1': (1, {'channel_factor': '3', 'kernel_size': '3', 'stride': '1', 'padding': '0'}), 'dropout2': (2, {'p': '0.4'})}\n",
            "NN build successfull!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-4e57f99010fc>:6: DeprecationWarning: Sampling from a set deprecated\n",
            "since Python 3.9 and will be removed in a subsequent version.\n",
            "  text = random.sample(layers_set, length)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = NN(in_channels=1)\n",
        "model.layers = seq"
      ],
      "metadata": {
        "id": "e-aBaOcuRoZ9"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "rTt94vmSYjth",
        "outputId": "f6718b98-209a-474f-ba47-93b47e8a9bd8"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "NN(\n",
              "  (layers): Sequential(\n",
              "    (0): Sequential(\n",
              "      (0): Dropout2d(p=0.2, inplace=False)\n",
              "      (1): Conv2d(1, 3, kernel_size=(3, 3), stride=(1, 1))\n",
              "      (2): ReLU(inplace=True)\n",
              "      (3): Dropout2d(p=0.4, inplace=False)\n",
              "    )\n",
              "    (1): Flatten(start_dim=1, end_dim=-1)\n",
              "    (2): Linear(in_features=2028, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)"
      ],
      "metadata": {
        "id": "9o6oy5FNR6Rv"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "    train_losses = []\n",
        "    valid_losses = []\n",
        "    # TODO calculate metrics and return them after train\n",
        "    def CalcValLoss():\n",
        "        with torch.no_grad():\n",
        "            losses = []\n",
        "            for X, Y in valid_dataloader:\n",
        "                X = X.float().to(device)\n",
        "                Y = Y.float().to(device)\n",
        "                preds = model(X)\n",
        "                preds, _ = torch.max(preds,1)\n",
        "                loss = criterion(preds,Y)\n",
        "                losses.append(loss.item())\n",
        "            print(\"Valid Loss : {:.6f}\".format(torch.tensor(losses).mean()))\n",
        "            valid_losses.append(torch.tensor(losses).mean())\n",
        "\n",
        "    for i in range(1, epochs):\n",
        "        losses = []\n",
        "        for X, Y in tqdm(train_dataloader):\n",
        "            X = X.float().to(device)\n",
        "            Y = Y.float().to(device)\n",
        "            preds = model(X)\n",
        "            preds, _ = torch.max(preds,1)\n",
        "            loss = criterion(preds, Y)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(\"Train Loss : {:.6f}\".format(torch.tensor(losses).mean()))\n",
        "        train_losses.append(torch.tensor(losses).mean())\n",
        "    return train_losses, valid_losses"
      ],
      "metadata": {
        "id": "witJfYDfLjWI"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses, valid_losses = train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sdEHOqftRQjJ",
        "outputId": "801a92d3-7da4-477a-f708-34ff5a9f2e99"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 125/125 [00:00<00:00, 213.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 8579.939453\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 125/125 [00:00<00:00, 219.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 493.860870\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 125/125 [00:00<00:00, 224.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 493.228210\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 125/125 [00:00<00:00, 347.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 493.066498\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 125/125 [00:00<00:00, 359.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 492.938690\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 125/125 [00:00<00:00, 340.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 492.947876\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 125/125 [00:00<00:00, 339.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 492.833740\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 125/125 [00:00<00:00, 335.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 492.833679\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 125/125 [00:00<00:00, 350.72it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 492.908295\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BEAwDtaRwx1D"
      },
      "execution_count": 38,
      "outputs": []
    }
  ]
}